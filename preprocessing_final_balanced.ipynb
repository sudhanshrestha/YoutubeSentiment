{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import emoji as emoji_lib\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Comment</th>\n",
       "      <th>sentiment_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unhappy Bacon</td>\n",
       "      <td>My friends loved it and agreed with everything...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amadanman85</td>\n",
       "      <td>I vow never ever too see this Movie.</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A Chambers</td>\n",
       "      <td>The real world.... A thought experiment......🇬🇧</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Irisdew</td>\n",
       "      <td>I really enjoyed the movie and didn&amp;#39;t find...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ben</td>\n",
       "      <td>I hope she’s joking. It’s a movie about Barbie...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17500</th>\n",
       "      <td>Daz Hatz</td>\n",
       "      <td>It makes you want to puke doesn&amp;#39;t it.</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17501</th>\n",
       "      <td>4EVERSEEKING WISDOM</td>\n",
       "      <td>If Cleopatra was supposed to be so beautiful t...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17502</th>\n",
       "      <td>Senni</td>\n",
       "      <td>what a time we live in when the &amp;quot;Asterix ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17503</th>\n",
       "      <td>Krystal Myth</td>\n",
       "      <td>Egypt deserves to win their case against corpo...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17504</th>\n",
       "      <td>Hexx Kana</td>\n",
       "      <td>watching your video.. i came to ask myself a f...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17505 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Author                                            Comment   \n",
       "0            Unhappy Bacon  My friends loved it and agreed with everything...  \\\n",
       "1              Amadanman85               I vow never ever too see this Movie.   \n",
       "2               A Chambers    The real world.... A thought experiment......🇬🇧   \n",
       "3                  Irisdew  I really enjoyed the movie and didn&#39;t find...   \n",
       "4                      Ben  I hope she’s joking. It’s a movie about Barbie...   \n",
       "...                    ...                                                ...   \n",
       "17500             Daz Hatz          It makes you want to puke doesn&#39;t it.   \n",
       "17501  4EVERSEEKING WISDOM  If Cleopatra was supposed to be so beautiful t...   \n",
       "17502                Senni  what a time we live in when the &quot;Asterix ...   \n",
       "17503         Krystal Myth  Egypt deserves to win their case against corpo...   \n",
       "17504            Hexx Kana  watching your video.. i came to ask myself a f...   \n",
       "\n",
       "      sentiment_label  \n",
       "0            positive  \n",
       "1            negative  \n",
       "2             neutral  \n",
       "3            positive  \n",
       "4            positive  \n",
       "...               ...  \n",
       "17500        negative  \n",
       "17501        negative  \n",
       "17502         neutral  \n",
       "17503        negative  \n",
       "17504         neutral  \n",
       "\n",
       "[17505 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the dataset\n",
    "df = pd.read_csv('./youtube_comments_new.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Author                                            Comment   \n",
      "943      NaN  I saw the movie and found the plot lacking so ...  \\\n",
      "12246    NaN  ANOTHER thing that disney screwed up in star w...   \n",
      "\n",
      "      sentiment_label  \n",
      "943           neutral  \n",
      "12246        negative  \n"
     ]
    }
   ],
   "source": [
    "na_rows = df[df.isna().any(axis=1)]\n",
    "print(na_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Author, Comment, sentiment_label]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "df.dropna(inplace=True)\n",
    "na_rows = df[df.isna().any(axis=1)]\n",
    "print(na_rows)\n",
    "## no na values left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment_label\n",
       "negative    9140\n",
       "positive    4602\n",
       "neutral     3761\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment_label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    " \n",
    "# remove HTML entity codes. \n",
    "def remove_html_codes(text):\n",
    "    decoded_text = html.unescape(text)\n",
    "    return decoded_text\n",
    "\n",
    "df['Comment'] = df['Comment'].apply(remove_html_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Comment</th>\n",
       "      <th>sentiment_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unhappy Bacon</td>\n",
       "      <td>my friends loved it and agreed with everything...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amadanman85</td>\n",
       "      <td>i vow never ever too see this movie.</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A Chambers</td>\n",
       "      <td>the real world.... a thought experiment......🇬🇧</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Irisdew</td>\n",
       "      <td>i really enjoyed the movie and didn't find it ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ben</td>\n",
       "      <td>i hope she’s joking. it’s a movie about barbie...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17500</th>\n",
       "      <td>Daz Hatz</td>\n",
       "      <td>it makes you want to puke doesn't it.</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17501</th>\n",
       "      <td>4EVERSEEKING WISDOM</td>\n",
       "      <td>if cleopatra was supposed to be so beautiful t...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17502</th>\n",
       "      <td>Senni</td>\n",
       "      <td>what a time we live in when the \"asterix &amp; obe...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17503</th>\n",
       "      <td>Krystal Myth</td>\n",
       "      <td>egypt deserves to win their case against corpo...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17504</th>\n",
       "      <td>Hexx Kana</td>\n",
       "      <td>watching your video.. i came to ask myself a f...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17503 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Author                                            Comment   \n",
       "0            Unhappy Bacon  my friends loved it and agreed with everything...  \\\n",
       "1              Amadanman85               i vow never ever too see this movie.   \n",
       "2               A Chambers    the real world.... a thought experiment......🇬🇧   \n",
       "3                  Irisdew  i really enjoyed the movie and didn't find it ...   \n",
       "4                      Ben  i hope she’s joking. it’s a movie about barbie...   \n",
       "...                    ...                                                ...   \n",
       "17500             Daz Hatz              it makes you want to puke doesn't it.   \n",
       "17501  4EVERSEEKING WISDOM  if cleopatra was supposed to be so beautiful t...   \n",
       "17502                Senni  what a time we live in when the \"asterix & obe...   \n",
       "17503         Krystal Myth  egypt deserves to win their case against corpo...   \n",
       "17504            Hexx Kana  watching your video.. i came to ask myself a f...   \n",
       "\n",
       "      sentiment_label  \n",
       "0            positive  \n",
       "1            negative  \n",
       "2             neutral  \n",
       "3            positive  \n",
       "4            positive  \n",
       "...               ...  \n",
       "17500        negative  \n",
       "17501        negative  \n",
       "17502         neutral  \n",
       "17503        negative  \n",
       "17504         neutral  \n",
       "\n",
       "[17503 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert text to lower case\n",
    "df['Comment'] = df['Comment'].str.lower()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By taking a look at the data, we found some special cases to deal with\n",
    "1. url links\n",
    "2. emojis\n",
    "3. slangs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Comment</th>\n",
       "      <th>sentiment_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unhappy Bacon</td>\n",
       "      <td>my friends loved it and agreed with everything...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amadanman85</td>\n",
       "      <td>i vow never ever too see this movie.</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A Chambers</td>\n",
       "      <td>the real world.... a thought experiment......🇬🇧</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Irisdew</td>\n",
       "      <td>i really enjoyed the movie and didn't find it ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ben</td>\n",
       "      <td>i hope she’s joking. it’s a movie about barbie...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17500</th>\n",
       "      <td>Daz Hatz</td>\n",
       "      <td>it makes you want to puke doesn't it.</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17501</th>\n",
       "      <td>4EVERSEEKING WISDOM</td>\n",
       "      <td>if cleopatra was supposed to be so beautiful t...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17502</th>\n",
       "      <td>Senni</td>\n",
       "      <td>what a time we live in when the \"asterix &amp; obe...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17503</th>\n",
       "      <td>Krystal Myth</td>\n",
       "      <td>egypt deserves to win their case against corpo...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17504</th>\n",
       "      <td>Hexx Kana</td>\n",
       "      <td>watching your video.. i came to ask myself a f...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17503 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Author                                            Comment   \n",
       "0            Unhappy Bacon  my friends loved it and agreed with everything...  \\\n",
       "1              Amadanman85               i vow never ever too see this movie.   \n",
       "2               A Chambers    the real world.... a thought experiment......🇬🇧   \n",
       "3                  Irisdew  i really enjoyed the movie and didn't find it ...   \n",
       "4                      Ben  i hope she’s joking. it’s a movie about barbie...   \n",
       "...                    ...                                                ...   \n",
       "17500             Daz Hatz              it makes you want to puke doesn't it.   \n",
       "17501  4EVERSEEKING WISDOM  if cleopatra was supposed to be so beautiful t...   \n",
       "17502                Senni  what a time we live in when the \"asterix & obe...   \n",
       "17503         Krystal Myth  egypt deserves to win their case against corpo...   \n",
       "17504            Hexx Kana  watching your video.. i came to ask myself a f...   \n",
       "\n",
       "      sentiment_label  \n",
       "0            positive  \n",
       "1            negative  \n",
       "2             neutral  \n",
       "3            positive  \n",
       "4            positive  \n",
       "...               ...  \n",
       "17500        negative  \n",
       "17501        negative  \n",
       "17502         neutral  \n",
       "17503        negative  \n",
       "17504         neutral  \n",
       "\n",
       "[17503 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove url links\n",
    "def remove_links(text):\n",
    "    # match strings starting with 'http'\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # match strings start with '<' and end with '>'\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    return text\n",
    "\n",
    "df['Comment'] = df['Comment'].apply(remove_links)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'😂': 'Extreme happiness, laughter', '❤️': 'Love', '🤣': 'Hysterical laughter', '👍': 'Well done, good job, approval', '😭': 'Uncontrollable sadness, joy', '🙏': 'Prayer, thank you, high five', '😘': 'Kiss, love', '🥰': 'Love, affection', '😍': 'Love, adoration', '😊': 'Positive, happy', '🎉': 'Celebration, congratulations', '😁': 'Glowing, beaming, happy', '💕': 'Love is in the air', '🥺': 'Adoration, bashful, pleading', '😅': 'Relief, nerves, excitement', '🔥': 'Hot, excellent', '☺️': 'Happy, positive', '🤦': 'Frustrated, dumbfounded', '♥️': 'Love', '🤷': 'Indifference, unknowing', '🙄': 'Sarcasm, boredom', '😆': 'Excitement, laughter, joy', '🤗': 'Hugging (love and support), jazz hands (enthusiasm)', '😉': 'Joking, cheeky', '🎂': 'Celebration, birthday', '🤔': 'Ponder, question', '👏': 'Applause', '🙂': 'Happy (sometimes ironic)', '😳': 'Embarrassed, surprise, flattered', '🥳': 'Celebration, joy', '😎': 'Cool, confident', '👌': 'Okay, approval, correct', '💜': 'Love', '😔': 'Reflective, remorseful', '💪': 'Strength, fitness', '✨': 'Positive, happy, celebration', '💖': 'Glowing love', '👀': 'Seen, shifty, drama', '😋': 'Cheeky, poking fun, enjoying food', '😏': 'Mischief, flirting', '😢': 'Upset, pain', '👉': 'Look right', '💗': 'Love, increasing affection', '😩': 'Distressed, drained, deep enjoyment', '💯': '100 percent approval', '🌹': 'Romance, a special occasion', '💞': 'A whirlwind of love', '🎈': 'Celebration, congratulations', '💙': 'Love', '😃': 'Happy', '😡': 'Annoyed, angry', '💐': 'Appreciation, happy', '😜': 'Fun, joking, cheeky', '🙈': 'Hiding, cringing, disbelief', '🤞': 'Hopeful, good luck', '😄': 'Happy, amused', '🤤': 'Desire, delirious', '🙌': 'Celebration, success', '🤪': 'Silly, fun, goofy', '❣️': 'Emphasized love', '😀': 'Happy', '💋': 'Seductive kiss', '💀': 'Death, extreme reaction', '👇': 'Look down', '💔': 'Grief, loss, longing', '😌': 'Content, calm', '💓': 'Fluttering love', '🤩': 'Amazement, impressed, excitement', '🙃': 'Silly, ironic, sarcasm', '😬': 'Tense, nervous, awkward', '😱': 'Shock, scared', '😴': 'Tired, bored', '🤭': 'Laughter, cheekiness', '😐': 'Neutral, irritated, unamused', '🌞': 'Radiant, happy, positive', '😒': 'Annoyed, skeptic, irritated', '😇': 'Angelic, innocent', '🌸': 'Love, a special occasion', '😈': 'Cheeky, naughty, mischief', '🎶': 'Singing, music', '✌️': 'Peace, success', '🎊': 'Celebration, congratulations', '🥵': 'Overheating, flirty', '😞': 'Disappointed, upset, remorse', '💚': 'Love', '☀️': 'Sunny, warm, hot', '🖤': 'Love, sorrow', '💰': 'Money, wealth', '😚': 'Affection, romance', '👑': 'Royalty, success, praise', '🎁': 'Celebration, surprise', '💥': 'Explosion, surprise, excitement', '🙋': 'Wave, ask a question', '☹️': 'Sad, concern, disappointed', '😑': 'Bored, indifference', '🥴': 'Drunk, dazed, confused', '👈': 'Look left', '💩': 'Poo, bad, silly', '✅': 'Approval, verification', '👋': 'Hello, goodbye', '🤮': 'Disgust, ugly, physical illness', '😤': 'Success, pride, passion, annoyed', '🤢': 'Disgust, nausea', '🌟': 'Positive, success, admiration', '❗': 'Emphasis, excitement, shock', '😥': 'Sad, relief, concern', '🌈': 'Happy, love, pride', '💛': 'Love, friendship', '😝': 'Joking, fun, playful', '😫': 'Tired, overwhelmed, dislike', '😲': 'Shock, surprise', '🖕': 'Swearing, contempt, disrespect', '‼️': 'Emphasis, excitement, shock', '🔴': 'Stop, recording', '🌻': 'Nature, sun, appreciation', '🤯': 'Shock, amazed, mind-blown', '💃': 'Fun, positive, carefree', '👊': 'Punch, fist bump', '🤬': 'Angry, swearing', '🏃': 'Fitness, rushing', '😕': 'Confused, hesitation, disappointed', '👁️': 'Watching, seen', '⚡': 'Danger, energy, lightning', '☕': 'Coffee, tea, gossip', '🍀': 'Good luck', '💦': 'Hard work, sweat, water, fluids', '⭐': 'Positive, success, rating', '🦋': 'Beauty, transformation, freedom', '🤨': 'Skeptic, suspicious', '🌺': 'Love, beauty', '😹': 'Extreme happiness, laughter', '🤘': 'Rock on', '🌷': 'Spring, nature', '💝': 'Love, gratitude, gift', '💤': 'Sleep, tired, bored', '🤝': 'Agree, respect', '🐰': 'Cute, rabbit, Easter', '😓': 'Sad, frustrated, disappointed', '💘': \"Love, Valentine's Day\", '🍻': 'Alcohol, beer, toasting', '😟': 'Worry, anxiety, disappointed', '😣': 'Frustrated, struggling, helpless', '🧐': 'Ponder, observe, smug', '😠': 'Anger, upset', '🤠': 'Adventure, confidence, empowered', '😻': 'Cute, love', '🌙': 'Mystery, night, sleep', '😛': 'Joking, fun, teasing', '🤙': 'Call me, shaka sign (friendly greeting)', '🙊': 'Mischief, secret, silence'}\n"
     ]
    }
   ],
   "source": [
    "# read the emojis.txt file\n",
    "with open('emojis.txt', 'r', encoding='utf-8') as file:\n",
    "    emoji_content = file.read()\n",
    "\n",
    "# create emoji dictionary from emojis.txt file\n",
    "emoji_dict = {}\n",
    "\n",
    "for line in emoji_content.strip().split(\"\\n\"):\n",
    "    emoji, meaning = line.split(\": \", 1)\n",
    "    emoji_dict[emoji] = meaning\n",
    "\n",
    "print(emoji_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Comment</th>\n",
       "      <th>sentiment_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unhappy Bacon</td>\n",
       "      <td>my friends loved it and agreed with everything...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amadanman85</td>\n",
       "      <td>i vow never ever too see this movie.</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A Chambers</td>\n",
       "      <td>the real world.... a thought experiment......:...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Irisdew</td>\n",
       "      <td>i really enjoyed the movie and didn't find it ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ben</td>\n",
       "      <td>i hope she’s joking. it’s a movie about barbie...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17500</th>\n",
       "      <td>Daz Hatz</td>\n",
       "      <td>it makes you want to puke doesn't it.</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17501</th>\n",
       "      <td>4EVERSEEKING WISDOM</td>\n",
       "      <td>if cleopatra was supposed to be so beautiful t...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17502</th>\n",
       "      <td>Senni</td>\n",
       "      <td>what a time we live in when the \"asterix &amp; obe...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17503</th>\n",
       "      <td>Krystal Myth</td>\n",
       "      <td>egypt deserves to win their case against corpo...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17504</th>\n",
       "      <td>Hexx Kana</td>\n",
       "      <td>watching your video.. i came to ask myself a f...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17503 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Author                                            Comment   \n",
       "0            Unhappy Bacon  my friends loved it and agreed with everything...  \\\n",
       "1              Amadanman85               i vow never ever too see this movie.   \n",
       "2               A Chambers  the real world.... a thought experiment......:...   \n",
       "3                  Irisdew  i really enjoyed the movie and didn't find it ...   \n",
       "4                      Ben  i hope she’s joking. it’s a movie about barbie...   \n",
       "...                    ...                                                ...   \n",
       "17500             Daz Hatz              it makes you want to puke doesn't it.   \n",
       "17501  4EVERSEEKING WISDOM  if cleopatra was supposed to be so beautiful t...   \n",
       "17502                Senni  what a time we live in when the \"asterix & obe...   \n",
       "17503         Krystal Myth  egypt deserves to win their case against corpo...   \n",
       "17504            Hexx Kana  watching your video.. i came to ask myself a f...   \n",
       "\n",
       "      sentiment_label  \n",
       "0            positive  \n",
       "1            negative  \n",
       "2             neutral  \n",
       "3            positive  \n",
       "4            positive  \n",
       "...               ...  \n",
       "17500        negative  \n",
       "17501        negative  \n",
       "17502         neutral  \n",
       "17503        negative  \n",
       "17504         neutral  \n",
       "\n",
       "[17503 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use python emoji library to deal with emoji\n",
    "# reference: https://carpedm20.github.io/emoji/docs/index.html\n",
    "# the converted text is sth like :smiley_face:\n",
    "# need to remove ':' and replace '_' with ' '\n",
    "\n",
    "def convert_emojis(text):\n",
    "    for em, meaning in emoji_dict.items():\n",
    "        text = text.replace(em, meaning)\n",
    "    return emoji_lib.demojize(text)\n",
    "\n",
    "df['Comment'] = df['Comment'].apply(convert_emojis)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Comment</th>\n",
       "      <th>sentiment_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unhappy Bacon</td>\n",
       "      <td>friend loved agreed everything movie trying sa...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amadanman85</td>\n",
       "      <td>vow never ever see movie</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A Chambers</td>\n",
       "      <td>real world .... thought experiment ...... Unit...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Irisdew</td>\n",
       "      <td>really enjoyed movie n't find overly feminist ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ben</td>\n",
       "      <td>hope ’ joking ’ movie barbie ’ woman highlight...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17500</th>\n",
       "      <td>Daz Hatz</td>\n",
       "      <td>make want puke n't</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17501</th>\n",
       "      <td>4EVERSEEKING WISDOM</td>\n",
       "      <td>cleopatra supposed beautiful pick ugly actress</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17502</th>\n",
       "      <td>Senni</td>\n",
       "      <td>time live `` asterix obelix '' depicted histor...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17503</th>\n",
       "      <td>Krystal Myth</td>\n",
       "      <td>egypt deserves win case corporate netflix woul...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17504</th>\n",
       "      <td>Hexx Kana</td>\n",
       "      <td>watching video .. came ask question ... 1 `` e...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17503 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Author                                            Comment   \n",
       "0            Unhappy Bacon  friend loved agreed everything movie trying sa...  \\\n",
       "1              Amadanman85                           vow never ever see movie   \n",
       "2               A Chambers  real world .... thought experiment ...... Unit...   \n",
       "3                  Irisdew  really enjoyed movie n't find overly feminist ...   \n",
       "4                      Ben  hope ’ joking ’ movie barbie ’ woman highlight...   \n",
       "...                    ...                                                ...   \n",
       "17500             Daz Hatz                                 make want puke n't   \n",
       "17501  4EVERSEEKING WISDOM     cleopatra supposed beautiful pick ugly actress   \n",
       "17502                Senni  time live `` asterix obelix '' depicted histor...   \n",
       "17503         Krystal Myth  egypt deserves win case corporate netflix woul...   \n",
       "17504            Hexx Kana  watching video .. came ask question ... 1 `` e...   \n",
       "\n",
       "      sentiment_label  \n",
       "0            positive  \n",
       "1            negative  \n",
       "2             neutral  \n",
       "3            positive  \n",
       "4            positive  \n",
       "...               ...  \n",
       "17500        negative  \n",
       "17501        negative  \n",
       "17502         neutral  \n",
       "17503        negative  \n",
       "17504         neutral  \n",
       "\n",
       "[17503 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess(text):\n",
    "    # tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # stop words removal\n",
    "    stop_words_removed = [word for word in tokens if word not in stopwords.words('english')]\n",
    "\n",
    "    # remove punctuation\n",
    "    punctuation_removed = [word for word in stop_words_removed if word not in list(string.punctuation)]\n",
    "\n",
    "    # lemmatization\n",
    "    lemmatized_text = [WordNetLemmatizer().lemmatize(word) for word in punctuation_removed]\n",
    "\n",
    "    return ' '.join(lemmatized_text)\n",
    "\n",
    "df['Comment'] = df['Comment'].apply(preprocess)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slangs dictionary\n",
    "# reference: https://www.kaggle.com/code/nmaguette/up-to-date-list-of-slangs-for-text-preprocessing\n",
    "slang_dict = {\n",
    "    \"$\" : \" dollar \",\n",
    "    \"€\" : \" euro \",\n",
    "    \"4ao\" : \"for adults only\",\n",
    "    \"a.m\" : \"before midday\",\n",
    "    \"a3\" : \"anytime anywhere anyplace\",\n",
    "    \"aamof\" : \"as a matter of fact\",\n",
    "    \"acct\" : \"account\",\n",
    "    \"adih\" : \"another day in hell\",\n",
    "    \"afaic\" : \"as far as i am concerned\",\n",
    "    \"afaict\" : \"as far as i can tell\",\n",
    "    \"afaik\" : \"as far as i know\",\n",
    "    \"afair\" : \"as far as i remember\",\n",
    "    \"afk\" : \"away from keyboard\",\n",
    "    \"app\" : \"application\",\n",
    "    \"approx\" : \"approximately\",\n",
    "    \"apps\" : \"applications\",\n",
    "    \"asap\" : \"as soon as possible\",\n",
    "    \"asl\" : \"age, sex, location\",\n",
    "    \"atk\" : \"at the keyboard\",\n",
    "    \"ave.\" : \"avenue\",\n",
    "    \"aymm\" : \"are you my mother\",\n",
    "    \"ayor\" : \"at your own risk\", \n",
    "    \"b&b\" : \"bed and breakfast\",\n",
    "    \"b+b\" : \"bed and breakfast\",\n",
    "    \"b.c\" : \"before christ\",\n",
    "    \"b2b\" : \"business to business\",\n",
    "    \"b2c\" : \"business to customer\",\n",
    "    \"b4\" : \"before\",\n",
    "    \"b4n\" : \"bye for now\",\n",
    "    \"b@u\" : \"back at you\",\n",
    "    \"bae\" : \"before anyone else\",\n",
    "    \"bak\" : \"back at keyboard\",\n",
    "    \"bbbg\" : \"bye bye be good\",\n",
    "    \"bbc\" : \"british broadcasting corporation\",\n",
    "    \"bbias\" : \"be back in a second\",\n",
    "    \"bbl\" : \"be back later\",\n",
    "    \"bbs\" : \"be back soon\",\n",
    "    \"be4\" : \"before\",\n",
    "    \"bfn\" : \"bye for now\",\n",
    "    \"blvd\" : \"boulevard\",\n",
    "    \"bout\" : \"about\",\n",
    "    \"brb\" : \"be right back\",\n",
    "    \"bros\" : \"brothers\",\n",
    "    \"brt\" : \"be right there\",\n",
    "    \"bsaaw\" : \"big smile and a wink\",\n",
    "    \"btw\" : \"by the way\",\n",
    "    \"bwl\" : \"bursting with laughter\",\n",
    "    \"c/o\" : \"care of\",\n",
    "    \"cet\" : \"central european time\",\n",
    "    \"cf\" : \"compare\",\n",
    "    \"cia\" : \"central intelligence agency\",\n",
    "    \"csl\" : \"can not stop laughing\",\n",
    "    \"cu\" : \"see you\",\n",
    "    \"cul8r\" : \"see you later\",\n",
    "    \"cv\" : \"curriculum vitae\",\n",
    "    \"cwot\" : \"complete waste of time\",\n",
    "    \"cya\" : \"see you\",\n",
    "    \"cyt\" : \"see you tomorrow\",\n",
    "    \"dae\" : \"does anyone else\",\n",
    "    \"dbmib\" : \"do not bother me i am busy\",\n",
    "    \"diy\" : \"do it yourself\",\n",
    "    \"dm\" : \"direct message\",\n",
    "    \"dwh\" : \"during work hours\",\n",
    "    \"e123\" : \"easy as one two three\",\n",
    "    \"eet\" : \"eastern european time\",\n",
    "    \"eg\" : \"example\",\n",
    "    \"embm\" : \"early morning business meeting\",\n",
    "    \"encl\" : \"enclosed\",\n",
    "    \"encl.\" : \"enclosed\",\n",
    "    \"etc\" : \"and so on\",\n",
    "    \"faq\" : \"frequently asked questions\",\n",
    "    \"fawc\" : \"for anyone who cares\",\n",
    "    \"fb\" : \"facebook\",\n",
    "    \"fc\" : \"fingers crossed\",\n",
    "    \"fig\" : \"figure\",\n",
    "    \"fimh\" : \"forever in my heart\", \n",
    "    \"ft.\" : \"feet\",\n",
    "    \"ft\" : \"featuring\",\n",
    "    \"ftl\" : \"for the loss\",\n",
    "    \"ftw\" : \"for the win\",\n",
    "    \"fwiw\" : \"for what it is worth\",\n",
    "    \"fyi\" : \"for your information\",\n",
    "    \"g9\" : \"genius\",\n",
    "    \"gahoy\" : \"get a hold of yourself\",\n",
    "    \"gal\" : \"get a life\",\n",
    "    \"gcse\" : \"general certificate of secondary education\",\n",
    "    \"gfn\" : \"gone for now\",\n",
    "    \"gg\" : \"good game\",\n",
    "    \"gl\" : \"good luck\",\n",
    "    \"glhf\" : \"good luck have fun\",\n",
    "    \"gmt\" : \"greenwich mean time\",\n",
    "    \"gmta\" : \"great minds think alike\",\n",
    "    \"gn\" : \"good night\",\n",
    "    \"g.o.a.t\" : \"greatest of all time\",\n",
    "    \"goat\" : \"greatest of all time\",\n",
    "    \"goi\" : \"get over it\",\n",
    "    \"gps\" : \"global positioning system\",\n",
    "    \"gr8\" : \"great\",\n",
    "    \"gratz\" : \"congratulations\",\n",
    "    \"gyal\" : \"girl\",\n",
    "    \"h&c\" : \"hot and cold\",\n",
    "    \"hp\" : \"horsepower\",\n",
    "    \"hr\" : \"hour\",\n",
    "    \"hrh\" : \"his royal highness\",\n",
    "    \"ht\" : \"height\",\n",
    "    \"ibrb\" : \"i will be right back\",\n",
    "    \"ic\" : \"i see\",\n",
    "    \"icq\" : \"i seek you\",\n",
    "    \"icymi\" : \"in case you missed it\",\n",
    "    \"idc\" : \"i do not care\",\n",
    "    \"idgadf\" : \"i do not give a damn fuck\",\n",
    "    \"idgaf\" : \"i do not give a fuck\",\n",
    "    \"idk\" : \"i do not know\",\n",
    "    \"ie\" : \"that is\",\n",
    "    \"i.e\" : \"that is\",\n",
    "    \"ifyp\" : \"i feel your pain\",\n",
    "    \"IG\" : \"instagram\",\n",
    "    \"iirc\" : \"if i remember correctly\",\n",
    "    \"ilu\" : \"i love you\",\n",
    "    \"ily\" : \"i love you\",\n",
    "    \"imho\" : \"in my humble opinion\",\n",
    "    \"imo\" : \"in my opinion\",\n",
    "    \"imu\" : \"i miss you\",\n",
    "    \"iow\" : \"in other words\",\n",
    "    \"irl\" : \"in real life\",\n",
    "    \"j4f\" : \"just for fun\",\n",
    "    \"jic\" : \"just in case\",\n",
    "    \"jk\" : \"just kidding\",\n",
    "    \"jsyk\" : \"just so you know\",\n",
    "    \"l8r\" : \"later\",\n",
    "    \"lb\" : \"pound\",\n",
    "    \"lbs\" : \"pounds\",\n",
    "    \"ldr\" : \"long distance relationship\",\n",
    "    \"lmao\" : \"laugh my ass off\",\n",
    "    \"lmfao\" : \"laugh my fucking ass off\",\n",
    "    \"lol\" : \"laughing out loud\",\n",
    "    \"ltd\" : \"limited\",\n",
    "    \"ltns\" : \"long time no see\",\n",
    "    \"m8\" : \"mate\",\n",
    "    \"mf\" : \"motherfucker\",\n",
    "    \"mfs\" : \"motherfuckers\",\n",
    "    \"mfw\" : \"my face when\",\n",
    "    \"mofo\" : \"motherfucker\",\n",
    "    \"mph\" : \"miles per hour\",\n",
    "    \"mr\" : \"mister\",\n",
    "    \"mrw\" : \"my reaction when\",\n",
    "    \"ms\" : \"miss\",\n",
    "    \"mte\" : \"my thoughts exactly\",\n",
    "    \"nagi\" : \"not a good idea\",\n",
    "    \"nbc\" : \"national broadcasting company\",\n",
    "    \"nbd\" : \"not big deal\",\n",
    "    \"nfs\" : \"not for sale\",\n",
    "    \"ngl\" : \"not going to lie\",\n",
    "    \"nhs\" : \"national health service\",\n",
    "    \"nrn\" : \"no reply necessary\",\n",
    "    \"nsfl\" : \"not safe for life\",\n",
    "    \"nsfw\" : \"not safe for work\",\n",
    "    \"nth\" : \"nice to have\",\n",
    "    \"nvr\" : \"never\",\n",
    "    \"nyc\" : \"new york city\",\n",
    "    \"oc\" : \"original content\",\n",
    "    \"og\" : \"original\",\n",
    "    \"ohp\" : \"overhead projector\",\n",
    "    \"oic\" : \"oh i see\",\n",
    "    \"omdb\" : \"over my dead body\",\n",
    "    \"omg\" : \"oh my god\",\n",
    "    \"omw\" : \"on my way\",\n",
    "    \"p.a\" : \"per annum\",\n",
    "    \"p.m\" : \"after midday\",\n",
    "    \"pm\" : \"prime minister\",\n",
    "    \"poc\" : \"people of color\",\n",
    "    \"pov\" : \"point of view\",\n",
    "    \"pp\" : \"pages\",\n",
    "    \"ppl\" : \"people\",\n",
    "    \"prw\" : \"parents are watching\",\n",
    "    \"ps\" : \"postscript\",\n",
    "    \"pt\" : \"point\",\n",
    "    \"ptb\" : \"please text back\",\n",
    "    \"pto\" : \"please turn over\",\n",
    "    \"qpsa\" : \"what happens\", #\"que pasa\",\n",
    "    \"ratchet\" : \"rude\",\n",
    "    \"rbtl\" : \"read between the lines\",\n",
    "    \"rlrt\" : \"real life retweet\", \n",
    "    \"rofl\" : \"rolling on the floor laughing\",\n",
    "    \"roflol\" : \"rolling on the floor laughing out loud\",\n",
    "    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n",
    "    \"rt\" : \"retweet\",\n",
    "    \"ruok\" : \"are you ok\",\n",
    "    \"sfw\" : \"safe for work\",\n",
    "    \"sk8\" : \"skate\",\n",
    "    \"smh\" : \"shake my head\",\n",
    "    \"sq\" : \"square\",\n",
    "    \"srsly\" : \"seriously\", \n",
    "    \"ssdd\" : \"same stuff different day\",\n",
    "    \"tbh\" : \"to be honest\",\n",
    "    \"tbs\" : \"tablespooful\",\n",
    "    \"tbsp\" : \"tablespooful\",\n",
    "    \"tfw\" : \"that feeling when\",\n",
    "    \"thks\" : \"thank you\",\n",
    "    \"tho\" : \"though\",\n",
    "    \"thx\" : \"thank you\",\n",
    "    \"tia\" : \"thanks in advance\",\n",
    "    \"til\" : \"today i learned\",\n",
    "    \"tl;dr\" : \"too long i did not read\",\n",
    "    \"tldr\" : \"too long i did not read\",\n",
    "    \"tmb\" : \"tweet me back\",\n",
    "    \"tntl\" : \"trying not to laugh\",\n",
    "    \"ttyl\" : \"talk to you later\",\n",
    "    \"u\" : \"you\",\n",
    "    \"u2\" : \"you too\",\n",
    "    \"u4e\" : \"yours for ever\",\n",
    "    \"utc\" : \"coordinated universal time\",\n",
    "    \"w/\" : \"with\",\n",
    "    \"w/o\" : \"without\",\n",
    "    \"w8\" : \"wait\",\n",
    "    \"wassup\" : \"what is up\",\n",
    "    \"wb\" : \"welcome back\",\n",
    "    \"wtf\" : \"what the fuck\",\n",
    "    \"wtg\" : \"way to go\",\n",
    "    \"wtpa\" : \"where the party at\",\n",
    "    \"wuf\" : \"where are you from\",\n",
    "    \"wuzup\" : \"what is up\",\n",
    "    \"wywh\" : \"wish you were here\",\n",
    "    \"yd\" : \"yard\",\n",
    "    \"ygtr\" : \"you got that right\",\n",
    "    \"ynk\" : \"you never know\",\n",
    "    \"zzz\" : \"sleeping bored and tired\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Comment</th>\n",
       "      <th>sentiment_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unhappy Bacon</td>\n",
       "      <td>friend loved agreed everything movie trying sa...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amadanman85</td>\n",
       "      <td>vow never ever see movie</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A Chambers</td>\n",
       "      <td>real world .... thought experiment ...... Unit...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Irisdew</td>\n",
       "      <td>really enjoyed movie n't find overly feminist ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ben</td>\n",
       "      <td>hope ’ joking ’ movie barbie ’ woman highlight...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17500</th>\n",
       "      <td>Daz Hatz</td>\n",
       "      <td>make want puke n't</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17501</th>\n",
       "      <td>4EVERSEEKING WISDOM</td>\n",
       "      <td>cleopatra supposed beautiful pick ugly actress</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17502</th>\n",
       "      <td>Senni</td>\n",
       "      <td>time live `` asterix obelix '' depicted histor...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17503</th>\n",
       "      <td>Krystal Myth</td>\n",
       "      <td>egypt deserves win case corporate netflix woul...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17504</th>\n",
       "      <td>Hexx Kana</td>\n",
       "      <td>watching video .. came ask question ... 1 `` e...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17503 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Author                                            Comment   \n",
       "0            Unhappy Bacon  friend loved agreed everything movie trying sa...  \\\n",
       "1              Amadanman85                           vow never ever see movie   \n",
       "2               A Chambers  real world .... thought experiment ...... Unit...   \n",
       "3                  Irisdew  really enjoyed movie n't find overly feminist ...   \n",
       "4                      Ben  hope ’ joking ’ movie barbie ’ woman highlight...   \n",
       "...                    ...                                                ...   \n",
       "17500             Daz Hatz                                 make want puke n't   \n",
       "17501  4EVERSEEKING WISDOM     cleopatra supposed beautiful pick ugly actress   \n",
       "17502                Senni  time live `` asterix obelix '' depicted histor...   \n",
       "17503         Krystal Myth  egypt deserves win case corporate netflix woul...   \n",
       "17504            Hexx Kana  watching video .. came ask question ... 1 `` e...   \n",
       "\n",
       "      sentiment_label  \n",
       "0            positive  \n",
       "1            negative  \n",
       "2             neutral  \n",
       "3            positive  \n",
       "4            positive  \n",
       "...               ...  \n",
       "17500        negative  \n",
       "17501        negative  \n",
       "17502         neutral  \n",
       "17503        negative  \n",
       "17504         neutral  \n",
       "\n",
       "[17503 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# deal with slangs\n",
    "def convert_slang(text):\n",
    "    words = text.split()\n",
    "    corrected_words = [slang_dict.get(word, word) for word in words]\n",
    "    return ' '.join(corrected_words)\n",
    "\n",
    "df['Comment'] = df['Comment'].apply(convert_slang)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Comment</th>\n",
       "      <th>sentiment_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unhappy Bacon</td>\n",
       "      <td>friend loved agreed everything movie trying sa...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amadanman85</td>\n",
       "      <td>vow never ever see movie</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A Chambers</td>\n",
       "      <td>real world  thought experiment  UnitedKingdom</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Irisdew</td>\n",
       "      <td>really enjoyed movie nt find overly feminist  ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ben</td>\n",
       "      <td>hope  joking  movie barbie  woman highlight is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17500</th>\n",
       "      <td>Daz Hatz</td>\n",
       "      <td>make want puke nt</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17501</th>\n",
       "      <td>4EVERSEEKING WISDOM</td>\n",
       "      <td>cleopatra supposed beautiful pick ugly actress</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17502</th>\n",
       "      <td>Senni</td>\n",
       "      <td>time live  asterix obelix  depicted history ac...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17503</th>\n",
       "      <td>Krystal Myth</td>\n",
       "      <td>egypt deserves win case corporate netflix woul...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17504</th>\n",
       "      <td>Hexx Kana</td>\n",
       "      <td>watching video  came ask question  1  expert  ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17503 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Author                                            Comment   \n",
       "0            Unhappy Bacon  friend loved agreed everything movie trying sa...  \\\n",
       "1              Amadanman85                           vow never ever see movie   \n",
       "2               A Chambers      real world  thought experiment  UnitedKingdom   \n",
       "3                  Irisdew  really enjoyed movie nt find overly feminist  ...   \n",
       "4                      Ben  hope  joking  movie barbie  woman highlight is...   \n",
       "...                    ...                                                ...   \n",
       "17500             Daz Hatz                                  make want puke nt   \n",
       "17501  4EVERSEEKING WISDOM     cleopatra supposed beautiful pick ugly actress   \n",
       "17502                Senni  time live  asterix obelix  depicted history ac...   \n",
       "17503         Krystal Myth  egypt deserves win case corporate netflix woul...   \n",
       "17504            Hexx Kana  watching video  came ask question  1  expert  ...   \n",
       "\n",
       "      sentiment_label  \n",
       "0            positive  \n",
       "1            negative  \n",
       "2             neutral  \n",
       "3            positive  \n",
       "4            positive  \n",
       "...               ...  \n",
       "17500        negative  \n",
       "17501        negative  \n",
       "17502         neutral  \n",
       "17503        negative  \n",
       "17504         neutral  \n",
       "\n",
       "[17503 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove special characters, return only letters, numbers and whitespaces\n",
    "def remove_special_characters(text):\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "df['Comment'] = df['Comment'].apply(remove_special_characters)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Comment</th>\n",
       "      <th>sentiment_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unhappy Bacon</td>\n",
       "      <td>friend loved agreed everything movie trying sa...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amadanman85</td>\n",
       "      <td>vow never ever see movie</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A Chambers</td>\n",
       "      <td>real world  thought experiment  UnitedKingdom</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ben</td>\n",
       "      <td>hope  joking  movie barbie  woman highlight is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ordinary Citizen</td>\n",
       "      <td>movie sound like social psyop s designed insta...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17499</th>\n",
       "      <td>magnus horus</td>\n",
       "      <td>video entertaining well made  cathartic listen...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17500</th>\n",
       "      <td>Daz Hatz</td>\n",
       "      <td>make want puke nt</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17501</th>\n",
       "      <td>4EVERSEEKING WISDOM</td>\n",
       "      <td>cleopatra supposed beautiful pick ugly actress</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17502</th>\n",
       "      <td>Senni</td>\n",
       "      <td>time live  asterix obelix  depicted history ac...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17503</th>\n",
       "      <td>Krystal Myth</td>\n",
       "      <td>egypt deserves win case corporate netflix woul...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16202 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Author                                            Comment   \n",
       "0            Unhappy Bacon  friend loved agreed everything movie trying sa...  \\\n",
       "1              Amadanman85                           vow never ever see movie   \n",
       "2               A Chambers      real world  thought experiment  UnitedKingdom   \n",
       "4                      Ben  hope  joking  movie barbie  woman highlight is...   \n",
       "5         Ordinary Citizen  movie sound like social psyop s designed insta...   \n",
       "...                    ...                                                ...   \n",
       "17499         magnus horus  video entertaining well made  cathartic listen...   \n",
       "17500             Daz Hatz                                  make want puke nt   \n",
       "17501  4EVERSEEKING WISDOM     cleopatra supposed beautiful pick ugly actress   \n",
       "17502                Senni  time live  asterix obelix  depicted history ac...   \n",
       "17503         Krystal Myth  egypt deserves win case corporate netflix woul...   \n",
       "\n",
       "      sentiment_label  \n",
       "0            positive  \n",
       "1            negative  \n",
       "2             neutral  \n",
       "4            positive  \n",
       "5            negative  \n",
       "...               ...  \n",
       "17499        positive  \n",
       "17500        negative  \n",
       "17501        negative  \n",
       "17502         neutral  \n",
       "17503        negative  \n",
       "\n",
       "[16202 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter comments between 10 and 500 chars in length\n",
    "df['comment_length'] = df['Comment'].apply(len)\n",
    "\n",
    "df.drop(df[(df['comment_length'] < 10) | (df['comment_length'] > 500)].index, inplace=True)\n",
    "\n",
    "df.drop(columns=['comment_length'], inplace=True)\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment_label\n",
       "negative    8545\n",
       "positive    4269\n",
       "neutral     3388\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your dataframe\n",
    "\n",
    "# Under-sampling negative class\n",
    "negative_sampled = df[df['sentiment_label'] == 'negative'].sample(4000)\n",
    "\n",
    "# Getting positive class\n",
    "positive = df[df['sentiment_label'] == 'positive'].sample(3500)\n",
    "\n",
    "# Getting neutral class\n",
    "neutral_sampled = df[df['sentiment_label'] == 'neutral']\n",
    "\n",
    "# Combining all\n",
    "balanced_df = pd.concat([negative_sampled, neutral_sampled, positive])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment_label\n",
       "negative    4000\n",
       "positive    3500\n",
       "neutral     3388\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment_label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('df_cleaned_new.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['processed_comment'] = df['Comment']\n",
    "df['labels'] = df['sentiment_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the data into training and testing sets (80-20 split)\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=11, stratify=df['labels'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train_data['processed_comment'])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_data['processed_comment'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "\n",
    "glove_model = api.load(\"glove-wiki-gigaword-100\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glove_embedding(comment):\n",
    "    words = comment.split()\n",
    "    embeddings = [glove_model[word] for word in words if word in glove_model.key_to_index]\n",
    "    \n",
    "    if not embeddings:\n",
    "        return np.zeros(glove_model.vector_size)\n",
    "    \n",
    "    return np.mean(embeddings, axis=0)\n",
    "\n",
    "X_train_glove = np.array([get_glove_embedding(comment) for comment in train_data['processed_comment']])\n",
    "X_test_glove = np.array([get_glove_embedding(comment) for comment in test_data['processed_comment']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased', return_dict=True).to(device)\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\ekyus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2393: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def get_bert_embedding(comment):\n",
    "    inputs = bert_tokenizer.encode_plus(\n",
    "        comment,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors='pt',\n",
    "        max_length=128,\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "    \n",
    "    # Move input tensors to the GPU\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
    "\n",
    "    # Extract the [CLS] token's embedding and move it back to the CPU\n",
    "    cls_embedding = outputs['last_hidden_state'][:, 0, :].squeeze().cpu().numpy()\n",
    "    return cls_embedding\n",
    "\n",
    "\n",
    "X_train_bert = np.array([get_bert_embedding(comment) for comment in train_data['processed_comment']])\n",
    "X_test_bert = np.array([get_bert_embedding(comment) for comment in test_data['processed_comment']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;, probability=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;, probability=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='linear', probability=True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Initialize the SVM classifier\n",
    "svm_classifier = SVC(kernel='linear', probability=True)\n",
    "\n",
    "# Train the classifier using the training data (TF-IDF representations)\n",
    "svm_classifier.fit(X_train_tfidf, train_data['labels'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.8408725602755454\n",
      "Testing Accuracy: 0.6515151515151515\n",
      "\n",
      "Training Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.86      0.84      3200\n",
      "     neutral       0.81      0.81      0.81      2710\n",
      "    positive       0.89      0.85      0.87      2800\n",
      "\n",
      "    accuracy                           0.84      8710\n",
      "   macro avg       0.84      0.84      0.84      8710\n",
      "weighted avg       0.84      0.84      0.84      8710\n",
      "\n",
      "\n",
      "Testing Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.65      0.68      0.66       800\n",
      "     neutral       0.57      0.60      0.58       678\n",
      "    positive       0.74      0.68      0.71       700\n",
      "\n",
      "    accuracy                           0.65      2178\n",
      "   macro avg       0.65      0.65      0.65      2178\n",
      "weighted avg       0.66      0.65      0.65      2178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict sentiments for training and testing data\n",
    "train_predictions = svm_classifier.predict(X_train_tfidf)\n",
    "test_predictions = svm_classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the classifier's performance\n",
    "train_accuracy = accuracy_score(train_data['labels'], train_predictions)\n",
    "test_accuracy = accuracy_score(test_data['labels'], test_predictions)\n",
    "train_report = classification_report(train_data['labels'], train_predictions)\n",
    "test_report = classification_report(test_data['labels'], test_predictions)\n",
    "\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "print(\"Testing Accuracy:\", test_accuracy)\n",
    "print(\"\\nTraining Classification Report:\\n\", train_report)\n",
    "print(\"\\nTesting Classification Report:\\n\", test_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;, probability=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;, probability=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='linear', probability=True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Initialize the SVM classifier\n",
    "svm_classifier_glove = SVC(kernel='linear', probability=True)\n",
    "\n",
    "# Train the classifier using the training data (GloVe representations)\n",
    "svm_classifier_glove.fit(X_train_glove, train_data['labels'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy (GloVe): 0.6420206659012629\n",
      "Testing Accuracy (GloVe): 0.6221303948576676\n",
      "\n",
      "Training Classification Report (GloVe):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.65      0.71      0.68      3200\n",
      "     neutral       0.60      0.55      0.57      2710\n",
      "    positive       0.67      0.66      0.67      2800\n",
      "\n",
      "    accuracy                           0.64      8710\n",
      "   macro avg       0.64      0.64      0.64      8710\n",
      "weighted avg       0.64      0.64      0.64      8710\n",
      "\n",
      "\n",
      "Testing Classification Report (GloVe):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.63      0.69      0.66       800\n",
      "     neutral       0.58      0.52      0.55       678\n",
      "    positive       0.65      0.64      0.65       700\n",
      "\n",
      "    accuracy                           0.62      2178\n",
      "   macro avg       0.62      0.62      0.62      2178\n",
      "weighted avg       0.62      0.62      0.62      2178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict sentiments for training and testing data\n",
    "train_predictions_glove = svm_classifier_glove.predict(X_train_glove)\n",
    "test_predictions_glove = svm_classifier_glove.predict(X_test_glove)\n",
    "\n",
    "# Evaluate the classifier's performance\n",
    "train_accuracy_glove = accuracy_score(train_data['labels'], train_predictions_glove)\n",
    "test_accuracy_glove = accuracy_score(test_data['labels'], test_predictions_glove)\n",
    "train_report_glove = classification_report(train_data['labels'], train_predictions_glove)\n",
    "test_report_glove = classification_report(test_data['labels'], test_predictions_glove)\n",
    "\n",
    "print(\"Training Accuracy (GloVe):\", train_accuracy_glove)\n",
    "print(\"Testing Accuracy (GloVe):\", test_accuracy_glove)\n",
    "print(\"\\nTraining Classification Report (GloVe):\\n\", train_report_glove)\n",
    "print(\"\\nTesting Classification Report (GloVe):\\n\", test_report_glove)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;, probability=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;, probability=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='linear', probability=True)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize another SVM classifier for BERT\n",
    "svm_classifier_bert = SVC(kernel='linear', probability=True)\n",
    "\n",
    "# Train the classifier using the training data (BERT representations)\n",
    "svm_classifier_bert.fit(X_train_bert, train_data['labels'])  # Note: Using train_data since we took a subset for BERT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy (BERT): 0.7451205510907003\n",
      "Testing Accuracy (BERT): 0.6253443526170799\n",
      "\n",
      "Training Classification Report (BERT):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.74      0.78      0.76      3200\n",
      "     neutral       0.73      0.69      0.71      2710\n",
      "    positive       0.77      0.76      0.77      2800\n",
      "\n",
      "    accuracy                           0.75      8710\n",
      "   macro avg       0.75      0.74      0.74      8710\n",
      "weighted avg       0.75      0.75      0.74      8710\n",
      "\n",
      "\n",
      "Testing Classification Report (BERT):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.65      0.69      0.67       800\n",
      "     neutral       0.56      0.51      0.54       678\n",
      "    positive       0.65      0.66      0.66       700\n",
      "\n",
      "    accuracy                           0.63      2178\n",
      "   macro avg       0.62      0.62      0.62      2178\n",
      "weighted avg       0.62      0.63      0.62      2178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict sentiments for training and testing data\n",
    "train_predictions_bert = svm_classifier_bert.predict(X_train_bert)\n",
    "test_predictions_bert = svm_classifier_bert.predict(X_test_bert)\n",
    "\n",
    "# Evaluate the classifier's performance\n",
    "train_accuracy_bert = accuracy_score(train_data['labels'], train_predictions_bert)\n",
    "test_accuracy_bert = accuracy_score(test_data['labels'], test_predictions_bert)\n",
    "train_report_bert = classification_report(train_data['labels'], train_predictions_bert)\n",
    "test_report_bert = classification_report(test_data['labels'], test_predictions_bert)\n",
    "\n",
    "print(\"Training Accuracy (BERT):\", train_accuracy_bert)\n",
    "print(\"Testing Accuracy (BERT):\", test_accuracy_bert)\n",
    "print(\"\\nTraining Classification Report (BERT):\\n\", train_report_bert)\n",
    "print(\"\\nTesting Classification Report (BERT):\\n\", test_report_bert)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from joblib import dump\n",
    "\n",
    "# # Save the model to a file\n",
    "# dump(svm_classifier_bert, 'svm_classifier_bert.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB Training Accuracy: 0.7690011481056257\n",
      "NB Testing Accuracy: 0.6345270890725436\n",
      "\n",
      "NB Training Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      0.86      0.79      3200\n",
      "     neutral       0.80      0.62      0.70      2710\n",
      "    positive       0.81      0.81      0.81      2800\n",
      "\n",
      "    accuracy                           0.77      8710\n",
      "   macro avg       0.78      0.76      0.76      8710\n",
      "weighted avg       0.78      0.77      0.77      8710\n",
      "\n",
      "\n",
      "NB Testing Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.60      0.79      0.68       800\n",
      "     neutral       0.63      0.40      0.49       678\n",
      "    positive       0.68      0.69      0.68       700\n",
      "\n",
      "    accuracy                           0.63      2178\n",
      "   macro avg       0.64      0.62      0.62      2178\n",
      "weighted avg       0.64      0.63      0.62      2178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Initialize the Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Train the classifier using the training data (TF-IDF representations)\n",
    "nb_classifier.fit(X_train_tfidf, train_data['labels'])\n",
    "\n",
    "# Predict sentiments for training and testing data\n",
    "train_predictions_nb = nb_classifier.predict(X_train_tfidf)\n",
    "test_predictions_nb = nb_classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the classifier's performance\n",
    "train_accuracy_nb = accuracy_score(train_data['labels'], train_predictions_nb)\n",
    "test_accuracy_nb = accuracy_score(test_data['labels'], test_predictions_nb)\n",
    "train_report_nb = classification_report(train_data['labels'], train_predictions_nb)\n",
    "test_report_nb = classification_report(test_data['labels'], test_predictions_nb)\n",
    "\n",
    "print(\"NB Training Accuracy:\", train_accuracy_nb)\n",
    "print(\"NB Testing Accuracy:\", test_accuracy_nb)\n",
    "print(\"\\nNB Training Classification Report:\\n\", train_report_nb)\n",
    "print(\"\\nNB Testing Classification Report:\\n\", test_report_nb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT Training Accuracy: 0.9986222732491389\n",
      "DT Testing Accuracy: 0.5532598714416896\n",
      "\n",
      "DT Training Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00      3200\n",
      "     neutral       1.00      1.00      1.00      2710\n",
      "    positive       1.00      1.00      1.00      2800\n",
      "\n",
      "    accuracy                           1.00      8710\n",
      "   macro avg       1.00      1.00      1.00      8710\n",
      "weighted avg       1.00      1.00      1.00      8710\n",
      "\n",
      "\n",
      "DT Testing Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.56      0.53      0.54       800\n",
      "     neutral       0.49      0.51      0.50       678\n",
      "    positive       0.61      0.62      0.62       700\n",
      "\n",
      "    accuracy                           0.55      2178\n",
      "   macro avg       0.55      0.55      0.55      2178\n",
      "weighted avg       0.55      0.55      0.55      2178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Initialize the Decision Tree classifier\n",
    "dt_classifier = DecisionTreeClassifier()\n",
    "\n",
    "# Train the classifier using the training data (TF-IDF representations)\n",
    "dt_classifier.fit(X_train_tfidf, train_data['labels'])\n",
    "\n",
    "# Predict sentiments for training and testing data\n",
    "train_predictions_dt = dt_classifier.predict(X_train_tfidf)\n",
    "test_predictions_dt = dt_classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the classifier's performance\n",
    "train_accuracy_dt = accuracy_score(train_data['labels'], train_predictions_dt)\n",
    "test_accuracy_dt = accuracy_score(test_data['labels'], test_predictions_dt)\n",
    "train_report_dt = classification_report(train_data['labels'], train_predictions_dt)\n",
    "test_report_dt = classification_report(test_data['labels'], test_predictions_dt)\n",
    "\n",
    "print(\"DT Training Accuracy:\", train_accuracy_dt)\n",
    "print(\"DT Testing Accuracy:\", test_accuracy_dt)\n",
    "print(\"\\nDT Training Classification Report:\\n\", train_report_dt)\n",
    "print(\"\\nDT Testing Classification Report:\\n\", test_report_dt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT Training Accuracy: 0.557060849598163\n",
      "DT Testing Accuracy: 0.5009182736455464\n",
      "\n",
      "DT Training Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.59      0.45      0.51      3200\n",
      "     neutral       0.44      0.76      0.56      2710\n",
      "    positive       0.88      0.48      0.62      2800\n",
      "\n",
      "    accuracy                           0.56      8710\n",
      "   macro avg       0.63      0.56      0.56      8710\n",
      "weighted avg       0.63      0.56      0.56      8710\n",
      "\n",
      "\n",
      "DT Testing Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.51      0.40      0.45       800\n",
      "     neutral       0.42      0.73      0.53       678\n",
      "    positive       0.74      0.40      0.52       700\n",
      "\n",
      "    accuracy                           0.50      2178\n",
      "   macro avg       0.56      0.51      0.50      2178\n",
      "weighted avg       0.56      0.50      0.50      2178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Initialize the Decision Tree classifier with hyperparameters to reduce overfitting\n",
    "dt_classifier = DecisionTreeClassifier(max_depth=15, min_samples_split=10, min_samples_leaf=5)\n",
    "\n",
    "# Train the classifier using the training data (TF-IDF representations)\n",
    "dt_classifier.fit(X_train_tfidf, train_data['labels'])\n",
    "\n",
    "# Predict sentiments for training and testing data\n",
    "train_predictions_dt = dt_classifier.predict(X_train_tfidf)\n",
    "test_predictions_dt = dt_classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the classifier's performance\n",
    "train_accuracy_dt = accuracy_score(train_data['labels'], train_predictions_dt)\n",
    "test_accuracy_dt = accuracy_score(test_data['labels'], test_predictions_dt)\n",
    "train_report_dt = classification_report(train_data['labels'], train_predictions_dt)\n",
    "test_report_dt = classification_report(test_data['labels'], test_predictions_dt)\n",
    "\n",
    "print(\"DT Training Accuracy:\", train_accuracy_dt)\n",
    "print(\"DT Testing Accuracy:\", test_accuracy_dt)\n",
    "print(\"\\nDT Training Classification Report:\\n\", train_report_dt)\n",
    "print(\"\\nDT Testing Classification Report:\\n\", test_report_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
